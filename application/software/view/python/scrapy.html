{include file="template/header" title="Python Scrapy" /}
<ol>
    <li class="summary closed">Scrapy安装</li>
    <div>
        <p>Twisted, PyWin32, Scrapy</p>
    </div>

    <li class="summary closed">确认安装</li>
    <div>
        <p>首先在Python中测试能否导入Scrapy模块:</p>
        <pre class="prettyprint">
>>> import scrapy
>>> scrapy.version_info
(1,5,0)</pre>
        <p>然后, 在shell中测试能否执行Scrapy这条命令:</p>
        <pre class="prettyprint">
$scrapy</pre>
    </div>

    <li class="summary closed">创建项目</li>
    <div>
        <p>首先, 在shell中使用scrapy startproject命令:</p>
        <pre class="prettyprint">
$scrapy startproject example</pre>
    </div>

    <li class="summary closed">实现Spider</li>
    <div>
        <p>实现爬虫的Python文件应位于example/spiders目录下, 在改目录下创建新文件book_spider.py. 然后在book_spider.py中实现爬虫BooksSpider, 代码如下:</p>
<xmp class="prettyprint"># -*- coding: utf-8 -*-
import scrapy

class BooksSpider(scrapy.Spider):
    # 每一个爬虫的唯一标识符
    name = "books"
    
    # 定义爬虫爬取的起始点, 起始点可以是多个, 这里只有一个
    start_urls = ['http://books.toscrape.com/']
    
    def parse(self, response):
        # 提取数据
        # 每一本书的信息在<article class="product_pod">中, 我们使用
        # css()方法找到所有这样的article元素, 并依次迭代
        for book in response.css('article.product_pod'):
            # 书名信息在article>h3>a元素的title属性里
            # 例如:<a title="A Light in the Attic">A Light in the...</a>
            name = book.xpath('./h3/a/@title').extract_first()
            
            # 书价信息在<p class="price_color">的TEXT中
            # 例如:<p class="price_color">$51.77</p>
            price = book.css('p.price_color::text').extract_first()
            yield {
                'name': name,
                'price': price,
            }
            
        # 提取链接
        # 下一页的url在ul.pager>li.next>a里面
        # 例如: <li class="next"><a href="catalogue/page-2.html">next</a></li>
        next_url = response.css('ul.pager li.next a::attr(href)').extract_first()
        if next_url:
            # 如果找到下一页的URL, 得到绝对路径, 构造新的Requset对象
            next_url = response.urljoin(next_url)
            yield scrapy.Request(next_url, callback = self.parse)</xmp>

        <ul>
            <li>name属性</li>
            <div>
                <p>一个Scrapy项目中可能有多个爬虫, 每个爬虫的name属性是其自身的唯一标识, 在一个项目中不能有同名的爬虫, 本例中的爬虫取名为'books'.</p>
            </div>

            <li>start_urls属性</li>
            <div>
                <p>一个爬虫总要从某个(或某些)页面开始爬取, 我们称这样的页面为起始爬取点.</p>
            </div>

            <li>parse方法</li>
            <div>
                <p>当一个页面下载完成后, Scrapy引擎会回掉一个我们制定的页面解析函数(默认为parse方法)解析页面. 一个页面解析函数通常需要完成以下两个任务:</p>
                <p>1) 提取页面中的数据(使用XPath或CSS选择器)</p>
                <p>2) 提取页面中的链接, 并产生对链接页面的下载请求</p>
            </div>
        </ul>
    </div>

    <li class="summary closed">运行爬虫</li>
    <div>
        <p>在shell中执行scrapy crawl SPIDER_NAME命令运行爬虫'books', 并将爬取的数据存储到csv文件中:</p>
<xmp class="prettyprint">$scrapy crawl books -o books.csv</xmp>
    </div>
</ol>
{include file="template/footer" /}